filter(!is.na(callback)) %>%
mutate(race=tolower(race)) %>%
mutate(gender=tolower(gender)) -> df_avg
df %>%
filter(study=="widner")
df %>%
mutate(race = replace_na(race, "non_white")) %>%
filter(study=="widner")
rm(list=ls())
library(tidyverse)
library(ggpubr)
library(ggExtra)
library(gt)
library(car)
library(haven)
library(meta)
library(metafor)
library(dmetar)
library(ragg)
wc<-read_csv("../0_data/ratings/names/df_all.csv",show_col_types = FALSE)
read_csv("../0_data/published_data/df_all.csv",show_col_types = FALSE) %>%
mutate(study=tolower(study)) %>%
mutate(callback_n=as.integer(callback_n))->cb
wc %>%
left_join(cb, by=c("study","name"), multiple="all") %>%
filter(!is.na(callback)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(competent)) -> df
cb %>%
left_join(wc %>%
group_by(study, name) %>%
summarise(warm=mean(warm), competent=mean(competent)),
by=c("study","name")) %>%
filter(!is.na(competent)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(callback)) %>%
mutate(race=tolower(race)) %>%
mutate(gender=tolower(gender)) -> df_avg
library(metaforest)
library(metafor)
library(tidyverse)
library(caret)
library(ggplot2)
set.seed(42)
df %>%
ungroup() %>%
select(warm, competent) ->df_temp2
pc <- prcomp(df_temp2,center = TRUE, scale. = TRUE)
s_pc<-summary(pc)
PC1<-pc[["x"]][,1] *-1
PC2<-pc[["x"]][,2] *-1
df %>%
mutate(race = replace_na(race, "non_white")) %>%
tibble::add_column(PC1,PC2) %>%
group_by(study, name, gender, race, callback,n) %>%
summarise(PC1=mean(PC1),
PC2=mean(PC2),.groups = "drop") %>%
mutate(callback=callback*100) %>%
mutate(vi = sqrt((callback*(100-callback))/ n)) %>% # calculate SE
ungroup() %>%
mutate(study=as.numeric(factor(study))) %>%
rename(id_exp=study,
yi=callback,
vi=)%>%
select(id_exp, yi, vi, PC1, race, gender) %>%
na.omit() ->df
check_conv <- MetaForest(yi~.,data = df,whichweights = "random",num.trees = 10000)
plt_1<-plot(check_conv)
plt_1
mf_rep <- MetaForest(yi~.,
data = df,
study = "id_exp",
whichweights = "random",
num.trees = 2500)
#---------------------------- select parameters
grouped_cv <- trainControl(method = "cv",
index = groupKFold(df$id_exp, k = 7))
tuning_grid <- expand.grid(whichweights = c("random"),
mtry = 1:3,
min.node.size = 1:3)
X <- df[, c("id_exp", "vi", "PC1", "gender", "race")]
mf_cv <- train(y = df$yi,
x = X,
study = "id_exp", # Name of the clustering variable
method = ModelInfo_mf(),
trControl = grouped_cv,
tuneGrid = tuning_grid,
num.trees = 3000)
mf_cv$results[which.min(mf_cv$results$RMSE), ]
#--------------- inspecting results
r2_cv <- mf_cv$results$Rsquared[which.min(mf_cv$results$RMSE)]
final <- mf_cv$finalModel
r2_oob <- final$forest$r.squared
plot(final)
VarImpPlot(final)
rm(list=ls())
library(bookdown)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(gapminder)
library(xtable)
library(stringr)
library(pander)
library(knitr)
library(scales)
library(RColorBrewer)
library(ordinal)
library(coin)
library(dtwclust)
library(dtwclust)
library(doParallel)
library(doParallel)
library(RColorBrewer)
library(ggpubr)
library(viridis)
library(rstatix)
library(kableExtra)
library(hrbrthemes)
library(viridis)
library(ggpmisc)
library(ggpmisc)
library(forcats)
library(kSamples)
library(dagitty)
library(lavaan)
library(MESS)
library(ggExtra)
library(ggsci)
library(scales)
#https://davidmathlogic.com/colorblind/#%23005AB5-%23DC3220
color_blind_friendly <- c("#FFB000", "#D35FB7", "#005AB5", "#DC3220", "#009E73")
color_blind_friendly2 <- c("#648FFF", "#DC267F","#785EF0","#FE6100", "#FFB000")
df_va_analytics<-rbind(read.csv("../experiment/data/0723/analytics.csv"),
read.csv("../experiment/data/0726/analytics.csv"),
read.csv("../experiment/data/0728/analytics.csv"),
read.csv("../experiment/data/0729/analytics.csv"))
rm(list=ls())
library(tidyverse)
library(ggpubr)
library(ggExtra)
library(gt)
library(car)
library(haven)
library(meta)
library(metafor)
library(dmetar)
library(ragg)
wc<-read_csv("../0_data/ratings/names/df_all.csv",show_col_types = FALSE)
read_csv("../0_data/published_data/df_all.csv",show_col_types = FALSE) %>%
mutate(study=tolower(study)) %>%
mutate(callback_n=as.integer(callback_n))->cb
wc %>%
left_join(cb, by=c("study","name"), multiple="all") %>%
filter(!is.na(callback)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(competent)) -> df
cb %>%
left_join(wc %>%
group_by(study, name) %>%
summarise(warm=mean(warm), competent=mean(competent)),
by=c("study","name")) %>%
filter(!is.na(competent)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(callback)) %>%
mutate(race=tolower(race)) %>%
mutate(gender=tolower(gender)) -> df_avg
library(metaforest)
library(metafor)
library(tidyverse)
library(caret)
library(ggplot2)
set.seed(42)
df %>%
ungroup() %>%
select(warm, competent) ->df_temp2
pc <- prcomp(df_temp2,center = TRUE, scale. = TRUE)
s_pc<-summary(pc)
PC1<-pc[["x"]][,1] *-1
PC2<-pc[["x"]][,2] *-1
df %>%
mutate(race = replace_na(race, "non_white")) %>%
tibble::add_column(PC1,PC2) %>%
group_by(study, name, gender, race, callback,n) %>%
summarise(PC1=mean(PC1),
PC2=mean(PC2),.groups = "drop") %>%
mutate(callback=callback*100) %>%
mutate(vi = sqrt((callback*(100-callback))/ n)) %>% # calculate SE
ungroup() %>%
mutate(study=as.numeric(factor(study))) %>%
rename(id_exp=study,
yi=callback,
vi=)%>%
select(id_exp, yi, vi, PC1, race, gender) %>%
na.omit() ->df
mf_rep <- MetaForest(yi~.,
data = df,
study = "id_exp",
whichweights = "random",
num.trees = 2500)
#---------------------------- select parameters
grouped_cv <- trainControl(method = "cv",
index = groupKFold(df$id_exp, k = 7))
unique(df$id_exp)
#---------------------------- select parameters
grouped_cv <- trainControl(method = "cv",
index = groupKFold(df$id_exp, k = 8))
tuning_grid <- expand.grid(whichweights = c("random"),
mtry = 1:3,
min.node.size = 1:3)
X <- df[, c("id_exp", "vi", "PC1", "gender", "race")]
mf_cv <- train(y = df$yi,
x = X,
study = "id_exp", # Name of the clustering variable
method = ModelInfo_mf(),
trControl = grouped_cv,
tuneGrid = tuning_grid,
num.trees = 3000)
mf_cv$results[which.min(mf_cv$results$RMSE), ]
#--------------- inspecting results
r2_cv <- mf_cv$results$Rsquared[which.min(mf_cv$results$RMSE)]
final <- mf_cv$finalModel
r2_oob <- final$forest$r.squared
plot(final)
VarImpPlot(final)
rm(list=ls())
library(tidyverse)
library(ggpubr)
library(ggExtra)
library(gt)
library(car)
library(haven)
library(meta)
library(metafor)
library(dmetar)
library(ragg)
wc<-read_csv("../0_data/ratings/names/df_all.csv",show_col_types = FALSE)
read_csv("../0_data/published_data/df_all.csv",show_col_types = FALSE) %>%
mutate(study=tolower(study)) %>%
mutate(callback_n=as.integer(callback_n))->cb
wc %>%
left_join(cb, by=c("study","name"), multiple="all") %>%
filter(!is.na(callback)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(competent)) -> df
cb %>%
left_join(wc %>%
group_by(study, name) %>%
summarise(warm=mean(warm), competent=mean(competent)),
by=c("study","name")) %>%
filter(!is.na(competent)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(callback)) %>%
mutate(race=tolower(race)) %>%
mutate(gender=tolower(gender)) -> df_avg
df %>%
filter(!study=="neumark")
unique(df$study)
"Estimate metaforest because for the linear model the predictions were really bad."
"https://cjvanlissa.github.io/metaforest/articles/Introduction_to_metaforest.html"
"https://dspace.library.uu.nl/bitstream/handle/1874/414894/10.4324_9780429273872_16_chapterpdf.pdf?sequence=1"
"run names qmd before!"
"new idea: Colin said we can check generalizability across categories by showing how important our predictors are"
"metaforest is a technique for variable selection; instead of running the usual metaregression (with which we were not successful)"
"we can estimate the metaforest with all predictor variables and then see which of those variables performs best!"
"this script takes a long time to run even with just 200 forests."
"this is actually a brilliant results; we need to replace table S8"
library(metaforest)
library(metafor)
library(tidyverse)
library(caret)
library(ggplot2)
set.seed(42)
df %>%
filter(!study=="neumark") %>% #excluded bc it does not vary race
ungroup() %>%
select(warm, competent) ->df_temp2
pc <- prcomp(df_temp2,center = TRUE, scale. = TRUE)
s_pc<-summary(pc)
PC1<-pc[["x"]][,1] *-1
PC2<-pc[["x"]][,2] *-1
df %>%
filter(!study=="neumark") %>% #excluded bc it does not vary race
mutate(race = replace_na(race, "non_white")) %>%
tibble::add_column(PC1,PC2) %>%
group_by(study, name, gender, race, callback,n) %>%
summarise(PC1=mean(PC1),
PC2=mean(PC2),.groups = "drop") %>%
mutate(callback=callback*100) %>%
mutate(vi = sqrt((callback*(100-callback))/ n)) %>% # calculate SE
ungroup() %>%
mutate(study=as.numeric(factor(study))) %>%
rename(id_exp=study,
yi=callback,
vi=)%>%
select(id_exp, yi, vi, PC1, race, gender) %>%
na.omit() ->df
check_conv <- MetaForest(yi~.,data = df,whichweights = "random",num.trees = 10000)
plt_1<-plot(check_conv)
plt_1
unique(df$id_exp)
#---------------------------- select parameters
grouped_cv <- trainControl(method = "cv",
index = groupKFold(df$id_exp, k =  unique(df$id_exp)))
#---------------------------- select parameters
grouped_cv <- trainControl(method = "cv",
index = groupKFold(df$id_exp, k =  7))
tuning_grid <- expand.grid(whichweights = c("random"),
mtry = 1:3,
min.node.size = 1:3)
X <- df[, c("id_exp", "vi", "PC1", "gender", "race")]
mf_cv <- train(y = df$yi,
x = X,
study = "id_exp", # Name of the clustering variable
method = ModelInfo_mf(),
trControl = grouped_cv,
tuneGrid = tuning_grid,
num.trees = 3000)
mf_cv$results[which.min(mf_cv$results$RMSE), ]
#--------------- inspecting results
r2_cv <- mf_cv$results$Rsquared[which.min(mf_cv$results$RMSE)]
final <- mf_cv$finalModel
r2_oob <- final$forest$r.squared
plot(final)
VarImpPlot(final)
ordered_vars <- names(final$forest$variable.importance)[
order(final$forest$variable.importance, decreasing = TRUE)]
PartialDependence(final, vars = ordered_vars, rawdata = TRUE, pi = .95)
wc<-read_csv("../0_data/ratings/names/df_all.csv",show_col_types = FALSE)
read_csv("../0_data/published_data/df_all.csv",show_col_types = FALSE) %>%
mutate(study=tolower(study)) %>%
mutate(callback_n=as.integer(callback_n))->cb
wc %>%
left_join(cb, by=c("study","name"), multiple="all") %>%
filter(!is.na(callback)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(competent)) -> df
cb %>%
left_join(wc %>%
group_by(study, name) %>%
summarise(warm=mean(warm), competent=mean(competent)),
by=c("study","name")) %>%
filter(!is.na(competent)) %>%
filter(!is.na(warm)) %>%
filter(!is.na(callback)) %>%
mutate(race=tolower(race)) %>%
mutate(gender=tolower(gender)) -> df_avg
df %>%
group_by(study) %>%
filter(all(c("black", "white") %in% race)) %>%
ungroup() %>%
select(warm, competent) ->df_temp2
pc <- prcomp(df_temp2,center = TRUE, scale. = TRUE)
s_pc<-summary(pc)
df_temp2
df %>%
group_by(study) %>%
filter(all(c("Black", "White") %in% race)) %>%
ungroup() %>%
select(warm, competent) ->df_temp2
df_temp2
df %>%
group_by(study) %>%
filter(all(c("Black", "White") %in% race)) %>%
ungroup() %>%
select(warm, competent) ->df_temp2
pc <- prcomp(df_temp2,center = TRUE, scale. = TRUE)
s_pc<-summary(pc)
PC1<-pc[["x"]][,1] *-1
PC2<-pc[["x"]][,2] *-1
df %>%
group_by(study) %>%
filter(all(c("Black", "White") %in% race)) %>%
ungroup() %>%
mutate(race = replace_na(race, "non_white")) %>%
tibble::add_column(PC1,PC2) %>%
group_by(study, name, gender, race, callback,n) %>%
summarise(PC1=mean(PC1),
PC2=mean(PC2),.groups = "drop") %>%
mutate(callback=callback*100) %>%
mutate(vi = sqrt((callback*(100-callback))/ n)) %>% # calculate SE
ungroup() %>%
mutate(study=as.numeric(factor(study))) %>%
rename(id_exp=study,
yi=callback,
vi=)%>%
select(id_exp, yi, vi, PC1, race, gender) %>%
na.omit() ->df
unique(df$id_exp)
check_conv <- MetaForest(yi~.,data = df,whichweights = "random",num.trees = 10000)
plt_1<-plot(check_conv)
plt_1
mf_rep <- MetaForest(yi~.,
data = df,
study = "id_exp",
whichweights = "random",
num.trees = 5000)
#---------------------------- select parameters
grouped_cv <- trainControl(method = "cv",
index = groupKFold(df$id_exp, k =  4)) # k-fold clustered cross-validation
tuning_grid <- expand.grid(whichweights = c("random"),
mtry = 1:3,
min.node.size = 1:3)
X <- df[, c("id_exp", "vi", "PC1", "gender", "race")]
mf_cv <- train(y = df$yi,
x = X,
study = "id_exp", # Name of the clustering variable
method = ModelInfo_mf(),
trControl = grouped_cv,
tuneGrid = tuning_grid,
num.trees = 5000)
mf_cv$results[which.min(mf_cv$results$RMSE), ]
#--------------- inspecting results
r2_cv <- mf_cv$results$Rsquared[which.min(mf_cv$results$RMSE)]
final <- mf_cv$finalModel
r2_oob <- final$forest$r.squared
plot(final)
VarImpPlot(final)
ordered_vars <- names(final$forest$variable.importance)[
order(final$forest$variable.importance, decreasing = TRUE)]
PartialDependence(final, vars = ordered_vars, rawdata = TRUE, pi = .95)
"we interestingly see that race and gender have negative variable importance;
this however could be the case because we include studies that do NOT have both."
plt_2<-plot(final)
plt_2
plt_3<-VarImpPlot(final)
plt_3
plt_4<-PartialDependence(final, vars = ordered_vars, rawdata = TRUE, pi = .95)
class(plt_4)
plt_4 <- ggplot2::last_plot()
plt_4
View(plt_4)
ordered_vars
PartialDependence(final, vars = PC1, rawdata = TRUE, pi = .95)
PartialDependence(final, vars = "PC1", rawdata = TRUE, pi = .95)
plt_4.1 <- ggplot2::last_plot()
ordered_vars
PartialDependence(final, vars = "race", rawdata = TRUE, pi = .95)
plt_4.2 <- ggplot2::last_plot()
PartialDependence(final, vars = "gender", rawdata = TRUE, pi = .95)
plt_4.3 <- ggplot2::last_plot()
library(ggpubr)
row1 <- ggarrange(plt_1, plt_2, plt_3, ncol = 3)
row2 <- ggarrange(plt_4.1, plt_4.2, plt_4.3, ncol = 3)
final_plot <- ggarrange(row1, row2, ncol = 1)
print(final_plot)
row1
row2
final_plot <- ggarrange(row1, row2, ncol = 1)
final_plot
plt_1
class(plt_1)
plt_1 +
geom_vline(aes(xintercept = 5000), color = "darkblue", linetype = "dotted") +
scale_x_continuous(breaks = c(0, 5000, 10000))
plt_1 +
geom_vline(aes(xintercept = 5000), color = "blue", linetype = "dotted") +
scale_x_continuous(breaks = c(0, 5000, 10000))
plt_1 +
geom_vline(aes(xintercept = 5000), color = "blue", linetype = "dotted") +
scale_x_continuous(breaks = c(0, 5000, 10000))+
ylim(10,12)
plt_1 +
geom_vline(aes(xintercept = 5000), color = "blue", linetype = "dotted") +
scale_x_continuous(breaks = c(0, 5000, 10000))+
ylim(10,11.5)
plt_1 +
geom_vline(aes(xintercept = 5000), color = "blue", linetype = "dotted") +
scale_x_continuous(breaks = c(0, 5000, 10000))+
ylim(10,11.5)->plt_1
plt_2<-plot(final)
plt_2
plot(final) + ylim(100,110)
plot(final) + ylim(100,110) + scale_x_continuous(breaks = c(0, 5000))
plot(final) + ylim(100,110) + scale_x_continuous(breaks = c(0, 5000))->plt_2
plt_3<-VarImpPlot(final)
plt_3
row1 <- ggarrange(plt_1, plt_2, plt_3, ncol = 3)
row2 <- ggarrange(plt_4.1, plt_4.2, plt_4.3, ncol = 3)
final_plot <- ggarrange(row1, row2, ncol = 1)
final_plot
plt_3
plt_3<-VarImpPlot(final) + xlab("Permutation Importance")
plt_3
plt_1 +
geom_vline(aes(xintercept = 5000), color = "blue", linetype = "dotted") +
scale_x_continuous(breaks = c(0, 5000, 10000))+
ylim(10,11.5)+xlab("# trees")->plt_1
plt_1
row1 <- ggarrange(plt_1, plt_2, plt_3, ncol = 3)
row2 <- ggarrange(plt_4.1, plt_4.2, plt_4.3, ncol = 3)
final_plot <- ggarrange(row1, row2, ncol = 1)
final_plot
plot(final) + ylim(100,110) + scale_x_continuous(breaks = c(0, 5000))+xlab("# trees")->plt_2
plot(final) + ylim(100,110) + scale_x_continuous(breaks = c(0, 5000))+xlab("# trees")+title("") ->plt_2
plot(final) + ylim(100,110) + scale_x_continuous(breaks = c(0, 5000))+xlab("# trees")+labs(title = NULL) ->plt_2
plt_2
plt_1 +
geom_vline(aes(xintercept = 5000), color = "blue", linetype = "dotted") +
scale_x_continuous(breaks = c(0, 5000, 10000))+
ylim(10,11.5)+xlab("# trees")+labs(title = NULL)->plt_1
row1 <- ggarrange(plt_1, plt_2, plt_3, ncol = 3)
row2 <- ggarrange(plt_4.1, plt_4.2, plt_4.3, ncol = 3)
final_plot <- ggarrange(row1, row2, ncol = 1)
final_plot
mf_cv$results[which.min(mf_cv$results$RMSE), ]
r2_oob <- final$forest$r.squared
r2_oob
r2_cv
ggarrange(row1, row2, ncol = 1)%>%
ggexport(filename = "metaforest.pdf")
